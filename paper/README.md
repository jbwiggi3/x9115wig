# Automatic Bug Handling
###Joseph B Wiggins
Final version will be submitted by 11:50

##Abstract
Bug reporting and handling of bug reports has become a critical piece of the software development process as we have moved toward development models with iterative development cycles. However, handling bug reports can be a particularly time intensive task, even before the developer gets to the code correction step. One task in particular that can be time consuming is deciding which bugs are duplicates of one another and usually has to be handled by one of the most experienced of the developers. Some companies have tried to promote policy that would eliminate the submission of duplicate bug reports but research suggests that duplicate bugs can be very helpful to the software development process and should be considered. In response, researchers have begun to experiment by building systems that automate triaging and other bug report related tasks to decrease the load on the developers. This literature review considers this line of work, displaying state-of-art attempts to solve these problems and the results. It also considers the body of work and suggests rooms for growth and improvement in future works.

##Keywords
Automatic Software Engineering, Triaging, Bug Reports, Machine Learning

##Introduction
Software development models have evolved as software engineering advanced as a field and the projects that were being developed increased in complexity. When most large scale coding was conducted for governmental agencies, the waterfall model was sufficient at handling the needs of developers. In the waterfall model, the details of the code are defined at the beginning of the process through communication with the clients and the software developers design and implement the code and hand it off to the client once the agreed upon functionality is completed. However, this approach falls apart when the goals of the software are ill-defined or exploratory (Nuseibeh, 2001). In today’s development world, software development is needed for many more types of tasks than in the past so more complex development models arose (Larman, & Basili, 2003). In one such model, the iterative development cycle, the developers cycle between planning, implementation, testing and evaluation continuously. 

In these new development models, a large portion of the development effort shifted from being completed before the code was released to after (Larman, & Basili, 2003). This development happens once deployed code starts revealing bugs. These bugs are discovered by end-users and end-users can report bugs to the software engineers via bug reports. However, these reports can become unruly once there is a high number of reports being submitted with a high velocity, which is often the case in today’s systems which cater to many users (Bettenburg, Premraj, Zimmermann, & Kim, 2008).

Each bug report that is submitted needs to be reviewed, combined with other bug reports like it, ranked based on it’s priority, and corrected in the code. The process of reviewing the bugs, especially identifying duplicates, or triaging, uses up a considerable amount of developer time. The more developer time the bug management task takes, the less availability the developers have for new functionalities or projects and can cut into a company's productivity (Sun, Lo, Khoo, & Jiang, 2011).

As the software engineering field has advanced, so have the tools for software development. Tools such as Bugzilla, a web interface for reporting bugs, are now publically available and help create some bug infrastructure for users (Serrano, & Ciordia, 2005). In their most basic forms, these systems act as a way to aggregate bug information and are helpful to software developers when addressing the bugs.

This literature review will consider the advancements in automatically handling the bug reports that are generated for code artifacts. This is especially important because of the advancements in software engineering and the emphasis placed on correcting bugs in systems.

##Related Work
In today’s computer science company, each system developed could be fostering to the needs of  millions of users. In order to keep up an image, a company needs to release code with as few bugs as possible, so quality assurance in software has become a large focus (Tian, 2005). The internally created bug reports by Quality Assurance staff are heavily scrutinized because it consumes developer resources to assign and correct the bugs (Anvik, Hiew, & Murphy, 2006). Many companies have policies against submitting duplicate bug reports because more triaging will have to be done by the developer and seen as lazy on the testers’ end (Bettenburg, Premraj, Zimmermann, & Kim, 2008).

However, people are starting to see duplicate bug reports in a different light. Bettenburg, Premraj, Zimmermann, & Kim (2008) took a hard look at duplicate bug reports and found many redeeming qualities which will be reviewed in the upcoming sections. The general consensus has become that duplicate bug reports improve developer awareness of defect and can aid in solving the defect, but are difficult to manage.

Since duplicates can be very helpful for high code quality, and that is the ultimate goal of a software company, the real question becomes: how can you consider all the bug reports without increasing the burden on the software developers? As computer scientists, it is natural to try to automate these processes. This paper serves as a guide through the body of literature surrounding duplicate bug reports and attempts to automate the processes around bug reporting. It will walk through the types of data that is used in these studies, a sample of the literature surrounding this work and a critical look at the problems that this body of literature has. It will conclude with some suggests about where the future of this body of work should move.

##Data Sets
An important element to consider in this body of literature is the data sets that are being used to evaluate new techniques. Many large software companies publically release their bug reports and they have been used in several of the papers mentioned in this article. Such pieces of software include Eclipse, the Java IDE, and Firefox, the web browser. However, while these data sets are common between papers, the information examined and the number of bugs considered during model building varies. See figure 1 for example elements contained by the bug reports. It is also important to note that whenever a paper references its performance (accuracy, F1, etc.) on triaging, it is comparing its results to the human triager.

Figure 1. Bug Report Components

##Automatically Handling Bug Reports
In this section we will talk about the bug reports, focusing on duplicates, and advancements made in automatic triaging. 

###Duplicate Bug Reports
In some companies, there are strict rules about writing a duplicate bug to one that is already in the system because it wastes developer resources. However, it is not always easy to tell if your bug has already been submitted and employees may not submit a bug when they are not sure. This culture may prevent bugs from being reported when they are discovered and perhaps prevent high quality bug reports. A paper by Bettenburg, Premraj, Zimmermann, and Kim (2008) considers bug reports and hopes to dispel the belief that duplicate bugs are harmful, and replace that with the belief that they are helpful to diagnosing the bug. Bettenburg, Premraj, Zimmermann, and Kim (2008) wanted to determine whether duplicate bug reports provided developers with more information than the original report and analyze that extra information. The most relevant to this paper is that some duplicate bug reports are not given because of lazy and inexperienced users, but instead because the same defect in the code is manifesting in multiple failures. Looking at these failures together gives a fuller picture of the defect and can give developers clues for correcting it.

A large portion of the development after software releases is based on the bug reports that the developers receive and the quality of those reports effects how quickly the problems can be fixed. However there is a disconnect between reporters and developers on what is important when reporting bugs and this is effecting productivity. Bettenburg, Just, and Schroter (2008) work suggests that after considering what both reporters and developers believe to be important in bug reports, tools can be developed which evaluate the quality of bug reports and then suggest improvements to the report. These improvements to the bug reports will help the bugs be addressed more quickly and effectively. Their contributions include responses from 466 developers and reporters on a survey about how bug reports are used, empirical evidence showing that bug reports do not usually meet the expectations and needs of developers, and a tool that evaluates bug reports and makes suggestions for how a reporter could improve the bug report.

###Triaging
With the knowledge that duplicate bug reports are helpful and details about what items are useful in bug reports, the research community recognized that it is important to maintain their duplicates and build infastructure to decrease the amount of work these extra bug reports put on the developers. Triaging is the practice of reviewing the bugs that are reported for a system and dividing them into master bugs, which are the original reports of the defect, and duplicates. Due to the potentially high number of bug reports a system can receive and the deep level of understanding of the software artifact that is required to triage, it is usually done by an experienced staff member. Since developer time is costly, many researchers have attempted to automate this process (Wang, Zhang, Xie, Anvik & Sun, 2008; Sureka & Jalote, 2010; Sun, Lo, Khoo, & Jiang, 2011; Nguyen et al., 2012).

Identification of duplicate bug reports automatically would allow for higher volume and velocity of bug reports to be addressed and that would result in increased possibility of revealing defects and quality of the software product. It also allows the software product to evolve to the user's believes about how it should work, increasing its ability to suit user needs. Wang, Zhang, Xie, Anvik, and Sun (2008) attempted this goal by adding in execution information, and found that duplicate bugs can be more accurately combined and save development time and improve product quality. They found a benefit in adding execution information and natural language information when combining bugs, provided a description of the approach for determining duplicate bugs, and empirical results concerning different heuristics.

When attempting to identify duplicates by considering high-level features, such as the words used in a bug report, automatic triaging is susceptible to dependence on a certain language, noisy data and handling domain specific term variations. A lower-level feature, such as character level features overcome these boundaries. Sureka and Jalote (2010) believed that character n-gram-based features would out-perform high-level feature based model for predicting duplicate bugs. In this paper, the authors took on significantly more bugs than their predecessors, moving from datasets of less than 50 thousand to 200 thousand, and found improvements in their models triaging abilities. This character level approach allows the ability to match concepts from system messages, extract super-word feature, handle misspelled words, match short-forms with corresponding expanded forms, match term variations to a common root and match hyphenated phrases. It also has the advantage that it can work on other languages than English without modifying the algorithms.

While this lexical was interesting it still lacked a strong source of information about the context of the bug. Sun, Lo, Khoo, and Jiang (2011) attempted to combine some new features into their machine learning models, aside from the bug text, to increase the accuracy of the automatic triaging. They brought in features such as product, component, version, priority, and type which can be gathered from most bug storing systems. They made a variant of BM25F which showed a 3-13% improvement in recall rate and a 4-11% improvement in MAP across four large datasets. They also created a new retrieval function, called REP, which outperformed their previous work with SVM by 10-27% on recall rate and 16-23% on MAP.

While the previous models provided some interesting gains in accuracy, they are not the same features the human triagers are able to exploit. When a human triager is reviewing a bug, they are able to get a deeper semantic understanding of the bug report that the previous papers have yet to touch on. A deeper semantic understanding would be able to understand connections between similar words and use that to connect the bugs. Nguyen et al. (2012) hypothesized building topic-based features would help capture the general idea of the bug and that idea will help connect duplicate bugs to each other. To do this, the authors then implemented DBTM (a duplicate bug report detection approach that takes advantage of the IR-based features and the topic based features). They found that DBTM had a 20% improvement in accuracy over the state-of-the-art approaches.

###Enhancing Bug Report Quality
Developers tag bugs with priorities so that they can do the most pressing tasks first and the least last, but this is a time intensive task, as it requires a knowledge of the implications the errors have on the system. Tian, Lo, and Sun (2013) hypothesized that by using machine learning algorithms, it should be possible to prioritize bugs automatically and increase the productivity of the software developers. In the 2013 paper, they created a new framework called DRONE (Predicting Priority via Multi-Faceted Factor Analyses) and a new classification engine called GRAY (Thresholding and Linear Regression to Classify Imbalanced Data). See figure X for an overview of the GRAY classification engine. In their system they used greedy hill climbing to tune their machine learning thresholds. The system was shown to outperform the baseline approaches in terms of average F-measure by a relative improvement of 58.61%.

Figure X. GRAY Classification Engine

Although methods for automatically triaging have become much more effective, it still leaves developers with several bugs and comments on bugs to review before they can proceed to try to fix the bugs. It could be very helpful if duplicate bug reports could also be automatically combined and summarized into a brief summary that the developer could read, gathering all relevant information and then be able to attack the bug, informed. Rastkar, Murphy, and Murray (2014) tackled this research problem by creating a system that was able to create extractive summaries, or summaries in which the contents of the summary are pulled directly from the parts that are being summarized. In their system, they found that not only was it possible to create these extractive summaries using bug reports and the comments on them, but that many of the participants who worked with the summaries reported preferring using them because they were ‘less intimidating/daunting’, ‘seemed more clear than full bug reports’, ‘were easier to read’, etc.

##Threats to validity
While reviewing this body of work, the author was careful to consider the potential threats to validity that could be in the papers. Many of the papers had individual Threats to Validity sections, so this section only focuses on Threats to validity in the body of work as a whole and individual paper's threats to validity can be viewed by referencing the papers. In this section, we will review conclusion validity, construct validity and external validity/transferability issues noted in these papers with the hopes that future works will also consider them.

###Conclusion validity – Does the treatment/change we introduced have a statistically significant effect on the outcome we measure?
When reviewing this body of literature it is important to consider if the results actually had a statistically significant effect. Some of the works represented here should be analyzed and reproduced before moving forward to confirm that the treatments actually had a statistically significant effect. In particular, several papers had small sample sizes, such as a sample size of 36 in the bug summarization work of (Rastkar, Murphy, & Murray, 2014).

###Construct validity – Does the treatment correspond to the actual cause we are interested in? Does the outcome correspond to the effect we are interested in?
Another important aspect is to consider if the study created treatments which are analogous to the real world bug reports and if the report results are actually the results we are interested in seeing. It is also important to consider the metrics that the papers are using to evaluate their systems and consider if such a measure is appropriate to evaluate the system. For instance, in Sun, Lo, Khoo, and Jiang’s (2011) work, we see reports for recall rate and MAP improvement, while in Bettenburg, Premraj, Zimmermann, and Kim report improvements in accuracy. However to truly evaluate the effectiveness of the system, we need to consider what the cost of different outcomes is. It may be the case that false positives are more detrimental than true negatives and we should optimize our processes towards minimizing the occurrence of false positives. 

###External validity/Transferability – Is the cause and effect relationship we have shown valid in other situations? Can we generalize our results? Do the results apply in other contexts?
There are also the generalizability issues that can arise in papers, in which the relationships that the papers displayed are not valid in other situations. It is also important to consider where models might be over-fitting to the domain the authors picked instead of being generalizable to bug reports in general. For instance, in Tian, Lo, and Sun’s (2014) work on predicting the priority of reported bugs, one of the inputs they used for their machine learning algorithm was the amount of bugs that the author of the bug had submitted. This assumption is a bit flawed, in that it assumes that who notices it has some effect on the priority of the bug, as if certain users only notice certain priority levels. This kind of variable being involved would also systematically discriminate against reports filed by casual bug reporters, and that could highly effect performance in some systems.

##Conclusions
As software engineering evolve, so must the processes that software engineers implement. In the paper, we have considered the importance of duplicate bugs, clarity of bug reports, reviewed automated process for triaging, prioritizing, and summarizing bugs, and we have considered the short-comings of the literature. It is important to consider the variety of ways that authors were able to make significant improvements in systems and imagine ways to combine the qualities of these methods to make the next generation of solution.

As this body of work advances, it will become signficantly easier for end-users to communicate their expectations of the code to the developers. Developers will then be able to automatically grab the higher priority items, already combined with their duplicates, and get straight to work instead of wading through a sea of bug reports and trying to isolate particular types of problems. Ultimately, these advances will reinforce the elements of iterative design and increase the quality of software projects.
	
##Future work
While this body of work has taken great steps forward, it is important to consider the shortcomings and future directions. In this future work section, we will consider what advancements can be made to automatic triaging, summarization, and then deployment of these intelligent bug report infastructures.

It is likely the the next place the community will look for improving the automatic triaging would be in deep learning, since that is a very popular topic at the moment and has been shown to be helpful in semantics. However, I also believe that through combining elements of the items reviewed by the various paper, it is likely that they will be able to produce high quality triaging results that are generalizable across domains. I am particularly interested in the combination of the deep semantic understanding of the topic modeling approach with the low level character analysis because I believe they both address shortcommings in one another.

Once automatic triaging has advanced, it will also be important to revisit the topic of automatic summarization. One short-coming of the Rastkar, Murphy, and Murray (2014) paper was that it was only capable of making extractive summaries, summaries in which the contents of the summary are pulled directly from the parts that are being summarized. However, a potentially much more effective tool would be one that instead created abstractive summaries, summaries in which the contents of the summary are generated based on a semantic representation of the things that are being summarized. As they did with their paper, it will be important to validate the abstractive summary system on software developers to assure that they find it informative and useful in resolving the bugs.

One important step for the future of this work is to make the tools publically available. The papers take advantage of already publically available tools, such as Bugzilla, to collect the bug reports. Automatic triaging could be built into tools like Bugzilla and then the algorithms suggested by these papers could be tested out in the field. Such an implementation would also be very helpful from a machine learning standpoint because it would increase the amount of bugs that the system is run on, and errors can be reported by users and considered when improving the algorithms.

##References (Need 20)
1.	Sun, C., Lo, D., Khoo, S.C., and Jiang, J. (2011). "Towards More Accurate Retrieval of Duplicate Bug Reports." Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE), 253-262.
2.	Wang, X., Zhang, L., Xie, T., Anvik, J., & Sun, J. (2008). "An Approach to Detecting Duplicate Bug Reports using Natural Language and Execution Information." Proceedings of the International Conference on Software Engineering, 461-470.
3.	Sureka, A. & Jalote, P. (2010). "Detecting Duplicate Bug Report Using Character N-Gram-Based Features." Proceedings of the 2010 Asia Pacific Software Engineering Conference, 366-374.
4.	Bettenburg, N., Just, S., & Schroter, A. (2008). "What Makes a Good Bug Report?" Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of software engineering, 308-318.
5.	Bettenburg, N., Premraj, R., Zimmermann, T., & Kim, S. (2008). "Duplicate bug reports considered harmful... really?" In ICSM '08: IEEE international conference on Software maintenance, 337-345.
6.	Tian, Y., Lo, D., & Sun, C. (2013, September). Drone: Predicting priority of reported bugs by multi-factor analysis. In Software Maintenance (ICSM), 2013 29th IEEE International Conference on (pp. 200-209). IEEE.
7.	Nguyen, A. T., Nguyen, T. T., Nguyen, T. N., Lo, D., & Sun, C. (2012, September). Duplicate bug report detection with a combination of information retrieval and topic modeling. In Automated Software Engineering (ASE), 2012 Proceedings of the 27th IEEE/ACM International Conference on (pp. 70-79). IEEE.
8.	Rastkar, S., Murphy, G. C., & Murray, G. (2014). Automatic summarization of bug reports. Software Engineering, IEEE Transactions on, 40(4), 366-380.
9.	Larman, C., & Basili, V. R. (2003). Iterative and incremental development: A brief history. Computer, (6), 47-56.
10.	Nuseibeh, B. (2001). Weaving together requirements and architectures. Computer, 34(3), 115-119.
11.	Serrano, N., & Ciordia, I. (2005). Bugzilla, ITracker, and other bug trackers. Software, IEEE, 22(2), 11-13.
12.	Tian, J. (2005). Software quality engineering: testing, quality assurance, and quantifiable improvement. John Wiley & Sons.
13.	Anvik, J., Hiew, L., & Murphy, G. C. (2006, May). Who should fix this bug?. In Proceedings of the 28th international conference on Software engineering (pp. 361-370). ACM.


